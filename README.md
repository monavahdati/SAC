# Discrete SAC Agent for BNPL Credit Strategy ğŸ’³

This project implements a Discrete Soft Actor-Critic (SAC) agent to optimize credit recommendations in a Buy Now, Pay Later (BNPL) system using reinforcement learning.

---

## ğŸ¯ Objective

Train a deep reinforcement learning agent that learns which credit offer to give to a user (or to reject) based on their features and historical behavior.

---

## ğŸ§  Model Components

### â–ª QNetwork
Approximates Q-values for each state-action pair. Two Q-networks (`q1`, `q2`) are used to reduce overestimation.

### â–ª PolicyNetwork
A softmax-based policy that outputs a distribution over actions. Used to sample actions and compute entropy-regularized policy loss.

### â–ª ReplayBuffer
Stores experiences `(state, action, reward, next_state, done)` for off-policy training.

### â–ª SimpleBNPLEnv
Simulates user credit behavior with defined rewards:
- âœ… Credit + Pay â†’ +2
- âŒ Credit + NoPay â†’ -1
- âŒ NoCredit + Pay â†’ -2
- âœ… NoCredit + NoPay â†’ +1

---

## âš™ï¸ Agent Training

- Entropy-regularized updates using Soft Actor-Critic formulation
- Target Q-networks updated with soft updates (Ï„ = 0.005)
- Optimizers: Adam with learning rate 3e-4
- Î³ (discount) = 0.99, Î± (entropy weight) = 0.2

---

## ğŸ“Š Metrics Tracked

- ğŸ”¹ Reward per episode
- ğŸ”¹ Training accuracy (credit decision vs true label)
- ğŸ”¹ Validation accuracy

---

## ğŸ“‚ Files

- `sac_model.py`: All models and agent
- `train.py`: Training loop
- `README.md`: Project documentation

---

## ğŸ“¦ Requirements

```bash
torch
numpy
scikit-learn
